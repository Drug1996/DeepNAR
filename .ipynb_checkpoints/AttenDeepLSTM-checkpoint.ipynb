{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from network import BiRNN, RNN, CNN, AttenBiRNN\n",
    "from visualization import variance_and_bias_analysis, plot_confusion_matrix, show, save, data_visual\n",
    "from data_optimization import dimension_reduce, normalization\n",
    "from data_augmentation import crop\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 0  # this parameter will be renewed in 'dataloader' function\n",
    "input_size = 36  # including 6 channels of 6 IMU sensors, totally 36 channels\n",
    "hidden_size = 256  # parameters for LSTM (Long Short Term Memory)\n",
    "num_layers = 2  # the depth of Deep-RNNs\n",
    "num_classes = 4\n",
    "batch_size = 48\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Test parameters\n",
    "RANDOM_SEED_NUM = 0\n",
    "TRIALS_NUM = 5\n",
    "LABELS = ['right_standing', 'wrong_standing', 'right_turning', 'wrong_turning']\n",
    "DATASET_NAME = 'dataset.pkl'\n",
    "\n",
    "\n",
    "def data_load():\n",
    "\n",
    "    dataset_name0 = 'lin'\n",
    "    dataset_name1 = 'zhong'\n",
    "    actions = LABELS\n",
    "    pkl_file = open(DATASET_NAME, 'rb')\n",
    "    dataset = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    dataset0 = dataset[dataset_name0]\n",
    "    dataset1 = dataset[dataset_name1]\n",
    "\n",
    "    # get longest length among different actions\n",
    "    max_t = max(dataset0['length_range'][1], dataset1['length_range'][1])\n",
    "    global sequence_length\n",
    "    sequence_length = max_t  # the longest length of sample\n",
    "\n",
    "    # divide the dataset into train set, dev set and test set\n",
    "    x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
    "    for label, action in enumerate(actions):\n",
    "\n",
    "        # lin\n",
    "        labels = [label] * len(dataset0[action])\n",
    "        temp_train_x, temp_test_x, temp_train_y, temp_test_y = \\\n",
    "            train_test_split(dataset0[action], labels, test_size=0.4, random_state=RANDOM_SEED_NUM)\n",
    "        x_train += temp_train_x\n",
    "        y_train += temp_train_y\n",
    "        temp_train_x, temp_test_x, temp_train_y, temp_test_y = \\\n",
    "            train_test_split(temp_test_x, temp_test_y, test_size=0.5, random_state=RANDOM_SEED_NUM)\n",
    "        x_dev += temp_train_x\n",
    "        y_dev += temp_train_y\n",
    "        x_test += temp_test_x\n",
    "        y_test +=temp_test_y\n",
    "\n",
    "        # zhong\n",
    "        labels = [label] * len(dataset1[action])\n",
    "        temp_train_x, temp_test_x, temp_train_y, temp_test_y = \\\n",
    "            train_test_split(dataset1[action], labels, test_size=0.4, random_state=RANDOM_SEED_NUM)\n",
    "        x_train += temp_train_x\n",
    "        y_train += temp_train_y\n",
    "        temp_train_x, temp_test_x, temp_train_y, temp_test_y = \\\n",
    "            train_test_split(temp_test_x, temp_test_y, test_size=0.5, random_state=RANDOM_SEED_NUM)\n",
    "        x_dev += temp_train_x\n",
    "        y_dev += temp_train_y\n",
    "        x_test += temp_test_x\n",
    "        y_test += temp_test_y\n",
    "\n",
    "    # data augmentation\n",
    "    x_train, y_train = crop(x_train, y_train)\n",
    "\n",
    "    # pad the data samples\n",
    "    for i in range(len(x_train)):\n",
    "        x_train[i] = \\\n",
    "            np.pad(x_train[i][:, 1:], ((0, max_t - x_train[i].shape[0]), (0, 0)), 'constant', constant_values=0)\n",
    "    for i in range(len(x_dev)):\n",
    "        x_dev[i] = \\\n",
    "            np.pad(x_dev[i][:, 1:], ((0, max_t - x_dev[i].shape[0]), (0, 0)), 'constant', constant_values=0)\n",
    "    for i in range(len(x_test)):\n",
    "        x_test[i] = \\\n",
    "            np.pad(x_test[i][:, 1:], ((0, max_t - x_test[i].shape[0]), (0, 0)), 'constant', constant_values=0)\n",
    "\n",
    "    # shuffle\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=RANDOM_SEED_NUM+1)\n",
    "    x_dev, y_dev = shuffle(x_dev, y_dev, random_state=RANDOM_SEED_NUM+2)\n",
    "    x_test, y_test = shuffle(x_test, y_test, random_state=RANDOM_SEED_NUM+3)\n",
    "\n",
    "    # change dataset's data format\n",
    "    x_train, y_train = \\\n",
    "        torch.from_numpy(np.array(x_train)).type(torch.FloatTensor),\\\n",
    "        torch.from_numpy(np.array(y_train)).type(torch.LongTensor)\n",
    "    x_dev, y_dev = \\\n",
    "        torch.from_numpy(np.array(x_dev)).type(torch.FloatTensor),\\\n",
    "        torch.from_numpy(np.array(y_dev)).type(torch.LongTensor)\n",
    "    x_test, y_test = \\\n",
    "        torch.from_numpy(np.array(x_test)).type(torch.FloatTensor),\\\n",
    "        torch.from_numpy(np.array(y_test)).type(torch.LongTensor)\n",
    "\n",
    "    return x_train, y_train, x_dev, y_dev, x_test, y_test\n",
    "\n",
    "\n",
    "def training_model(num):\n",
    "    # torch.manual_seed(RANDOM_SEED_NUM)\n",
    "\n",
    "    # Load the dataset\n",
    "    X_training, Y_training, X_dev, Y_dev, X_test, Y_test = data_load()\n",
    "    \n",
    "    # Record losses, accuracies and labels for drawing analysis figures\n",
    "    training_losses = []\n",
    "    test_accuracies = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Create the model\n",
    "    model = AttenBiRNN(input_size, hidden_size, num_layers, num_classes, sequence_length).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() # Cross Entropy Loss funtion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Adam optimzer\n",
    "\n",
    "    # Create Pytorch's dataloader for training, valid and test dataset\n",
    "    train_dataset = TensorDataset(X_training, Y_training)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataset = TensorDataset(X_dev, Y_dev)\n",
    "    dev_loader = DataLoader(dev_dataset)\n",
    "    test_dataset = TensorDataset(X_test, Y_test)\n",
    "    test_loader = DataLoader(test_dataset)\n",
    "\n",
    "    # Start training the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.reshape(-1).to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            training_loss = criterion(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Print information, collect the training loss and valid accuracy for every 2 epoches\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            # Print information\n",
    "            print('Test [{}], Epoch [{}/{}], Loss: {:.4f}'\n",
    "                  .format(num+1, epoch + 1, num_epochs, training_loss.item()))\n",
    "            # Get the value of loss\n",
    "            training_losses.append(training_loss.item())\n",
    "            # Test the model on valid dataset\n",
    "            with torch.no_grad():\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for j, (inputs, labels) in enumerate(dev_loader):\n",
    "                    inputs = inputs.reshape(-1, sequence_length, input_size).to(device)\n",
    "                    labels = labels.reshape(-1).to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    y_true.append(labels.item())\n",
    "                    y_pred.append(predicted.item())\n",
    "                test_accuracies.append(correct/total)\n",
    "\n",
    "    # Print accuracy of the model\n",
    "    accuracy = test_accuracies[-1]*100\n",
    "    print('Dev accuracy of the No.{} model on dev action samples: {} %'.format(num+1, accuracy))\n",
    "\n",
    "    # Show or save the graph of variance and bias analysis, and confusion matrix graph\n",
    "    variance_and_bias_analysis([training_losses], [test_accuracies])\n",
    "    save('trials' + str(num+1) + '_loss_accuracy' + '.png')\n",
    "    plot_confusion_matrix([y_true], [y_pred], LABELS)\n",
    "    save('trials_' + str(num+1) + '_confusion_matrix' + '.png')\n",
    "\n",
    "    return accuracy, model, test_loader\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    # Test the model on test set\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.reshape(-1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true.append(labels.item())\n",
    "            y_pred.append(predicted.item())\n",
    "\n",
    "        print('Final accuracy is {} %'.format((correct/total)*100))\n",
    "        plot_confusion_matrix([y_true], [y_pred], LABELS)\n",
    "        save('test_confusion_matrix' + '.png')\n",
    "\n",
    "    # Save the modal checkpoint\n",
    "    torch.save(model.state_dict(), 'DeepNAR_model.ckpt')\n",
    "\n",
    "\n",
    "def main(num):\n",
    "    accuracylist = []\n",
    "    modellist = []\n",
    "    for i in range(num):\n",
    "        accuracy, model, test_loader = training_model(i)\n",
    "        accuracylist.append(accuracy)\n",
    "        modellist.append(model)\n",
    "    index = accuracylist.index(max(accuracylist))\n",
    "    print('Choose No.{} model as test model with {} % Dev accuracy.'.format(index+1, max(accuracylist)))\n",
    "    best_model = modellist[index]\n",
    "    test_model(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6c62583b10f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRIALS_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cost time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-68d87d79af11>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mmodellist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0maccuracylist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mmodellist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-68d87d79af11>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttenBiRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/DeepNAR/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, num_layers, num_classes, max_len)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 2 for bidirection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/DeepNAR/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_dim, step_dim, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "main(num=TRIALS_NUM)\n",
    "end = datetime.datetime.now()\n",
    "print('Cost time: ', end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
